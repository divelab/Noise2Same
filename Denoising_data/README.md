Denoising Datasets used in our experiments
-----

### Dataset download
Due to the limitation of repo size, we upload the datasets to Google Drive. You can **download all the four datasets** [here](https://drive.google.com/drive/folders/1VYMo1OoaGxoOLNx6-qIt2Wg03lsZw_kA?usp=sharing).

### How are those datasets generated?

#### RGB Natural Images (ImageNet)
We construct the RGB natural image dataset from the ImageNet ILSVRC2012 Validation dataset that consists of 50,000 natural images. In particular, we follow [Noise2Self](https://arxiv.org/abs/1901.11365) to generate noisy images by applying a combination of three types of noises to the clear images. The noises are Poisson noise (λ = 30), additive Gaussian noise (μ = 0, σ = 60) and Bernoulli noise (p = 0.2). To be consistent to Noise2Self, we randomly crop 60,000 patches of size 128 x 128 from the first 20,000 images in ILSVRC2012 Val to construct the training dataset. Additional two sets of 1,000 images from ILSVRC2012 Val are used for validation and testing, respectively.

The code to generate the noisy images can be found in `Denoising_Data/ImageNet/generate_noisy.ipynb`.

#### Hand-written Chinese Character Images (Hanzi)
We generate the HànZì dataset with the code provided by [Noise2Self](https://arxiv.org/abs/1901.11365). The dataset is constructed with 13029 Chinese characters and consists of 78174 noisy images of size 64 x 64, where each noisy image is generated by applying Gaussian noise (σ = 0.7) and Bernoulli noise to a clear Chinese character image. Among the 78174 noisy images, 90% are used for training and validation, and the rest 10% are for testing.

The code to generate the noisy images is available at [this repo](https://github.com/batson/hanzi).

#### 3D Fluorescence Microscopy Data (CARE-Planaria)
In order to show the capability of our approach to 3D images with inconsistent and untypical noise, we use the physically acquired 3D fluorescence microscopy data collected from Planaria (Schmidtea mediterranea) provided by [CARE](https://www.nature.com/articles/s41592-018-0216-7). The training data, consisting of 17005 3D patches of size 16 x 64 x 64, is a mix of noisy images at three noise levels, collected under different conditions (C1, C2, and C3) of exposure time and laser power. The trained models are evaluated on 20 testing images of size 96 x 1024 x 1024 at three different noise levels individually. From condition 1 (C1) to condition 3 (C3), the noise gets stronger, and input image quality gets worse.

The original noisy image dataset is available [here](https://publications.mpi-cbg.de/publications-sites/7207/).

#### Grey-scale Natural Images (BSD68)
We follow [Noise2Void](https://arxiv.org/abs/1811.10980) to construct the BSD68 Dataset. For the training set, patches of size 180 x 180 are cropped from each of the 400 grey-scale version of images of range [0, 255], where each image is treated with Gaussian noise (σ = 25). The trained models are then evaluated on 68 testing images.

The original noisy image dataset can be also downloaded [here](https://cloud.mpi-cbg.de/index.php/s/pbj89sV6n6SyM29/download).
